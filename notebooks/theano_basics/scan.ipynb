{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Scan (Theano)\n",
    "\n",
    "To execute a cell: Ctrl-Enter.\n",
    "\n",
    "The code was executed with the default configuration of Theano: `floatX=float64`, `device=cpu` and the configuration for GPU `floatX=float32,device=cuda`. For additional information about `scan`, please refer to [theano doc](http://deeplearning.net/software/theano/library/scan.html).\n",
    "\n",
    "Tested with:\n",
    "- Python 3.6.2, \n",
    "- Theano 0.10.0beta1.dev,\n",
    "- Lasagne 0.2.dev1,\n",
    "- cuDNN version 6021,\n",
    "- GeForce GTX TITAN Black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['THEANO_FLAGS'] = 'floatX=float64,device=cpu,mode=FAST_RUN'\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32, device=cuda, mode=FAST_RUN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation graph and loop\n",
    "\n",
    "Let `A` be a tensor and `k` a positive integer, we are interested by computing `A.^k` (element-wise power). This computation involves a for loop that iterates `k` times. To define the associated computation graph, we use the `scan` function. The arguments of the `scan` function are the:\n",
    "- fn: a Theano function representing the computation that is done in a single iteration of the loop represented by the scan op. Note that **the order of parameters is fixed by** `scan`: the output of the prior call to `fn` is the first parameter, followed by all non-sequences,\n",
    "- output_info: initial value of the output variable,\n",
    "- sequence: A sequence is a Theano variable which Scan will iterate over and give sub-elements to its inner function as input. A sequence has no associated output. For a sequence variable X, at timestep t, the inner function will receive as input the sequence element X[t],\n",
    "- non_sequences: A non-sequence is a Theano variable which Scan will provide as-is to its inner function. Like a sequence, a non-sequence has no associated output. For a non-sequence variable X, at timestep t, the inner function will receive as input the variable X,\n",
    "- n_steps: number of iteration\n",
    "\n",
    "Then, `scan` returns a tuple containing our result and a dictionary of updates (empty in the following case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   1.   4.   9.  16.  25.  36.  49.  64.  81.]\n",
      "[  0.00000000e+00   1.00000000e+00   1.60000000e+01   8.10000000e+01\n",
      "   2.56000000e+02   6.25000000e+02   1.29600000e+03   2.40100000e+03\n",
      "   4.09600000e+03   6.56100000e+03]\n"
     ]
    }
   ],
   "source": [
    "k = T.iscalar(\"k\")\n",
    "A = T.vector(\"A\")\n",
    "\n",
    "# Symbolic description of the result\n",
    "result, updates = theano.scan(fn=lambda prior_result, A: prior_result * A,\n",
    "                              outputs_info=T.ones_like(A),\n",
    "                              non_sequences=A,\n",
    "                              n_steps=k)\n",
    "\n",
    "# We only care about A**k, but scan has provided us with A**1 through A**k.\n",
    "# Discard the values that we don't care about. Scan is smart enough to\n",
    "# notice this and not waste memory saving them.\n",
    "final_result = result[-1]\n",
    "\n",
    "# compiled function that returns A**k\n",
    "power = theano.function(inputs=[A,k], outputs=final_result, updates=updates)\n",
    "\n",
    "print(power(range(10),2))\n",
    "print(power(range(10),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With scan, it is important to distinguish the following types of output variables (tap refers to time slices of sequences or outputs):\n",
    "- Nitsot (no input tap, single output tap) : A nitsot is an output variable of the inner function that is not fed back as an input to the next iteration of the inner function. Nitsots are typically encountered in situations where Scan is used to perform a ‘map’ operation (every element in a tensor is independently altered using a given operation to produce a new tensor) such as squaring every number in a vector,\n",
    "- Sitsot (single input tap, single output tap) : A sitsot is an output variable of the inner function that is fed back as an input to the next iteration of the inner function. A typical setting where a sitsot might be encountered is the case where Scan is used to compute the cumulative sum over the elements of a vector and a sitsot output is employed to act as an accumulator,\n",
    "- Mitsot (multiple input taps, single output tap) : A mitsot is an output variable of the inner function that is fed back as an input to future iterations of the inner function (either multiple future iterations or a single one that isn’t the immediate next one). For example, a mitsot might be used in the case where Scan is used to compute the Fibonacci sequence, one term of the sequence at every timestep, since every computed term needs to be reused to compute the two next terms of the sequence,\n",
    "- Mitmot (multiple input taps, multiple output taps) : These outputs exist but they cannot be directly created by the user. They can appear in a theano graph as a result of taking the gradient of the output of a Scan with respect to its inputs: This will result in the creation of a new scan node used to compute the gradients of the first scan node. If the original Scan had sitsots or mitsots variables, the new Scan will use mitmots to compute the gradients through time for these variables.\n",
    "\n",
    "The first three types can be used by the user, while the last one is only used internally for computing the gradient through `scan`.\n",
    "In the next example, we calculate the polynomial by first generating each of the coefficients, and then summing them at the end. There is no accumulation of results so we can set `outputs_info` to `None`. This indicates to scan that it doesn’t need to pass the prior result to `fn`.\n",
    "\n",
    "The general order of function parameters to fn is:\n",
    "- sequences (if any),\n",
    "- prior result(s) (if needed), \n",
    "- non-sequences (if any).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.0\n",
      "19.0\n"
     ]
    }
   ],
   "source": [
    "coefficients = theano.tensor.vector(\"coefficients\")\n",
    "x = T.scalar(\"x\")\n",
    "\n",
    "max_coefficients_supported = 10000\n",
    "\n",
    "# Generate the components of the polynomial\n",
    "components, updates = theano.scan(fn=lambda coefficient, power, free_variable: coefficient * (free_variable ** power),\n",
    "                                  outputs_info=None,\n",
    "                                  sequences=[coefficients, theano.tensor.arange(max_coefficients_supported)],\n",
    "                                  non_sequences=x)\n",
    "# Sum them up\n",
    "polynomial = components.sum()\n",
    "\n",
    "# Compile a function\n",
    "calculate_polynomial = theano.function(inputs=[coefficients, x], outputs=polynomial)\n",
    "\n",
    "# Test\n",
    "test_coefficients = np.asarray([1, 0, 2], dtype=np.float32)\n",
    "test_value = 3\n",
    "print(calculate_polynomial(test_coefficients, test_value))\n",
    "print(1.0 * (3 ** 0) + 0.0 * (3 ** 1) + 2.0 * (3 ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
